{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ensemble bit",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DboPjz09LgT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import seaborn as sns\n",
        "sns.set_style('darkgrid')\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.preprocessing import PolynomialFeatures\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8Vvreu_BCNc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ocCg1MCBKip",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "335ee5a9-4fe5-46f0-cce6-c3045cdce00a"
      },
      "source": [
        "train.shape, test.shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4177, 363), (1000, 362))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Xm870uwBPHR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = train.append(test, ignore_index=True, sort=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izb519AXBWGX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "121d9098-69b2-4a81-b854-661761fe7c2c"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>span</th>\n",
              "      <th>target</th>\n",
              "      <th>feature_00</th>\n",
              "      <th>feature_01</th>\n",
              "      <th>feature_02</th>\n",
              "      <th>feature_03</th>\n",
              "      <th>feature_04</th>\n",
              "      <th>feature_05</th>\n",
              "      <th>feature_06</th>\n",
              "      <th>feature_07</th>\n",
              "      <th>feature_08</th>\n",
              "      <th>feature_09</th>\n",
              "      <th>feature_10</th>\n",
              "      <th>feature_11</th>\n",
              "      <th>feature_12</th>\n",
              "      <th>feature_13</th>\n",
              "      <th>feature_14</th>\n",
              "      <th>feature_15</th>\n",
              "      <th>feature_16</th>\n",
              "      <th>feature_17</th>\n",
              "      <th>feature_18</th>\n",
              "      <th>feature_19</th>\n",
              "      <th>feature_20</th>\n",
              "      <th>feature_21</th>\n",
              "      <th>feature_22</th>\n",
              "      <th>feature_23</th>\n",
              "      <th>feature_24</th>\n",
              "      <th>feature_25</th>\n",
              "      <th>feature_26</th>\n",
              "      <th>feature_27</th>\n",
              "      <th>feature_28</th>\n",
              "      <th>feature_29</th>\n",
              "      <th>feature_30</th>\n",
              "      <th>feature_31</th>\n",
              "      <th>feature_32</th>\n",
              "      <th>feature_33</th>\n",
              "      <th>feature_34</th>\n",
              "      <th>feature_35</th>\n",
              "      <th>feature_36</th>\n",
              "      <th>...</th>\n",
              "      <th>feature_20_type5</th>\n",
              "      <th>feature_21_type5</th>\n",
              "      <th>feature_22_type5</th>\n",
              "      <th>feature_23_type5</th>\n",
              "      <th>feature_24_type5</th>\n",
              "      <th>feature_25_type5</th>\n",
              "      <th>feature_26_type5</th>\n",
              "      <th>feature_27_type5</th>\n",
              "      <th>feature_28_type5</th>\n",
              "      <th>feature_29_type5</th>\n",
              "      <th>feature_30_type5</th>\n",
              "      <th>feature_31_type5</th>\n",
              "      <th>feature_32_type5</th>\n",
              "      <th>feature_33_type5</th>\n",
              "      <th>feature_34_type5</th>\n",
              "      <th>feature_35_type5</th>\n",
              "      <th>feature_36_type5</th>\n",
              "      <th>feature_37_type5</th>\n",
              "      <th>feature_38_type5</th>\n",
              "      <th>feature_39_type5</th>\n",
              "      <th>feature_40_type5</th>\n",
              "      <th>feature_41_type5</th>\n",
              "      <th>feature_42_type5</th>\n",
              "      <th>feature_43_type5</th>\n",
              "      <th>feature_44_type5</th>\n",
              "      <th>feature_45_type5</th>\n",
              "      <th>feature_46_type5</th>\n",
              "      <th>feature_47_type5</th>\n",
              "      <th>feature_48_type5</th>\n",
              "      <th>feature_49_type5</th>\n",
              "      <th>feature_50_type5</th>\n",
              "      <th>feature_51_type5</th>\n",
              "      <th>feature_52_type5</th>\n",
              "      <th>feature_53_type5</th>\n",
              "      <th>feature_54_type5</th>\n",
              "      <th>feature_55_type5</th>\n",
              "      <th>feature_56_type5</th>\n",
              "      <th>feature_57_type5</th>\n",
              "      <th>feature_58_type5</th>\n",
              "      <th>feature_59_type5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>M00yaXQwMVJ1R1Q0RHFBVVR5BHyNyIeEd7yqxidFW7U=</td>\n",
              "      <td>17</td>\n",
              "      <td>1.025179</td>\n",
              "      <td>1.455438</td>\n",
              "      <td>0.767232</td>\n",
              "      <td>1.103207</td>\n",
              "      <td>0.355392</td>\n",
              "      <td>0.144054</td>\n",
              "      <td>1.249518</td>\n",
              "      <td>0.301203</td>\n",
              "      <td>-1.470791</td>\n",
              "      <td>-0.934316</td>\n",
              "      <td>0.886449</td>\n",
              "      <td>-1.113141</td>\n",
              "      <td>1.049017</td>\n",
              "      <td>1.222423</td>\n",
              "      <td>0.788908</td>\n",
              "      <td>-1.541238</td>\n",
              "      <td>1.536722</td>\n",
              "      <td>-1.682130</td>\n",
              "      <td>-1.546657</td>\n",
              "      <td>-1.297385</td>\n",
              "      <td>0.333717</td>\n",
              "      <td>0.149473</td>\n",
              "      <td>0.729299</td>\n",
              "      <td>0.387906</td>\n",
              "      <td>-0.744653</td>\n",
              "      <td>1.341640</td>\n",
              "      <td>0.404163</td>\n",
              "      <td>-2.083132</td>\n",
              "      <td>-0.674207</td>\n",
              "      <td>1.157396</td>\n",
              "      <td>0.750975</td>\n",
              "      <td>0.870192</td>\n",
              "      <td>0.875611</td>\n",
              "      <td>0.593826</td>\n",
              "      <td>-0.603761</td>\n",
              "      <td>-0.294881</td>\n",
              "      <td>0.723880</td>\n",
              "      <td>0.994828</td>\n",
              "      <td>...</td>\n",
              "      <td>0.477301</td>\n",
              "      <td>0.858831</td>\n",
              "      <td>-1.725857</td>\n",
              "      <td>1.844273</td>\n",
              "      <td>-0.598755</td>\n",
              "      <td>0.930897</td>\n",
              "      <td>0.068039</td>\n",
              "      <td>-1.364287</td>\n",
              "      <td>0.886032</td>\n",
              "      <td>-0.308191</td>\n",
              "      <td>0.871548</td>\n",
              "      <td>0.697917</td>\n",
              "      <td>-0.663403</td>\n",
              "      <td>-1.169636</td>\n",
              "      <td>0.207404</td>\n",
              "      <td>-0.733880</td>\n",
              "      <td>-0.462923</td>\n",
              "      <td>-0.226763</td>\n",
              "      <td>0.305259</td>\n",
              "      <td>0.198925</td>\n",
              "      <td>0.564382</td>\n",
              "      <td>-0.416291</td>\n",
              "      <td>0.852825</td>\n",
              "      <td>-0.076624</td>\n",
              "      <td>0.528348</td>\n",
              "      <td>0.183028</td>\n",
              "      <td>-0.655101</td>\n",
              "      <td>-0.964741</td>\n",
              "      <td>-0.756489</td>\n",
              "      <td>1.559715</td>\n",
              "      <td>-1.395021</td>\n",
              "      <td>-1.222626</td>\n",
              "      <td>-0.139682</td>\n",
              "      <td>0.813436</td>\n",
              "      <td>-0.340162</td>\n",
              "      <td>-0.032112</td>\n",
              "      <td>1.740059</td>\n",
              "      <td>1.000138</td>\n",
              "      <td>-1.667038</td>\n",
              "      <td>-0.128554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>M01HU2pzN1dZNnVOYmhjayLgjCmIG1AyHvfRsZv+IV4=</td>\n",
              "      <td>23</td>\n",
              "      <td>0.996383</td>\n",
              "      <td>0.132304</td>\n",
              "      <td>1.724691</td>\n",
              "      <td>1.517504</td>\n",
              "      <td>0.884100</td>\n",
              "      <td>-0.897716</td>\n",
              "      <td>-0.122241</td>\n",
              "      <td>1.523423</td>\n",
              "      <td>-1.152261</td>\n",
              "      <td>-0.915475</td>\n",
              "      <td>-0.714206</td>\n",
              "      <td>0.043509</td>\n",
              "      <td>-0.222875</td>\n",
              "      <td>-0.483340</td>\n",
              "      <td>-0.737885</td>\n",
              "      <td>-0.501099</td>\n",
              "      <td>0.132304</td>\n",
              "      <td>-1.625833</td>\n",
              "      <td>-1.353529</td>\n",
              "      <td>-0.962832</td>\n",
              "      <td>0.469725</td>\n",
              "      <td>1.570780</td>\n",
              "      <td>-1.371288</td>\n",
              "      <td>1.612218</td>\n",
              "      <td>-0.317589</td>\n",
              "      <td>0.629555</td>\n",
              "      <td>-0.140000</td>\n",
              "      <td>-0.613572</td>\n",
              "      <td>0.860422</td>\n",
              "      <td>1.487905</td>\n",
              "      <td>-0.613572</td>\n",
              "      <td>1.511584</td>\n",
              "      <td>-0.353107</td>\n",
              "      <td>-1.122663</td>\n",
              "      <td>-0.240634</td>\n",
              "      <td>1.103128</td>\n",
              "      <td>-0.003848</td>\n",
              "      <td>1.274798</td>\n",
              "      <td>...</td>\n",
              "      <td>0.933283</td>\n",
              "      <td>-0.898041</td>\n",
              "      <td>1.064092</td>\n",
              "      <td>-2.211074</td>\n",
              "      <td>1.575640</td>\n",
              "      <td>-1.051326</td>\n",
              "      <td>-0.259730</td>\n",
              "      <td>0.053807</td>\n",
              "      <td>0.096736</td>\n",
              "      <td>0.970818</td>\n",
              "      <td>-0.963895</td>\n",
              "      <td>-0.228488</td>\n",
              "      <td>0.802025</td>\n",
              "      <td>0.387122</td>\n",
              "      <td>0.875071</td>\n",
              "      <td>-0.756669</td>\n",
              "      <td>-0.214778</td>\n",
              "      <td>1.657901</td>\n",
              "      <td>0.502198</td>\n",
              "      <td>0.418139</td>\n",
              "      <td>-0.127123</td>\n",
              "      <td>1.193327</td>\n",
              "      <td>0.517931</td>\n",
              "      <td>0.865856</td>\n",
              "      <td>1.022512</td>\n",
              "      <td>-0.282880</td>\n",
              "      <td>-0.770379</td>\n",
              "      <td>-0.185785</td>\n",
              "      <td>-0.769705</td>\n",
              "      <td>-1.371605</td>\n",
              "      <td>-1.269116</td>\n",
              "      <td>0.477700</td>\n",
              "      <td>1.590249</td>\n",
              "      <td>0.166410</td>\n",
              "      <td>0.648515</td>\n",
              "      <td>0.128202</td>\n",
              "      <td>-1.853485</td>\n",
              "      <td>-0.185335</td>\n",
              "      <td>-0.933777</td>\n",
              "      <td>0.420162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>M01la1Znd0ROMUU5UGJvdc5EJqBI/dj9r8C86lWkehA=</td>\n",
              "      <td>29</td>\n",
              "      <td>1.006573</td>\n",
              "      <td>0.542726</td>\n",
              "      <td>0.844240</td>\n",
              "      <td>-0.089719</td>\n",
              "      <td>0.204441</td>\n",
              "      <td>0.300044</td>\n",
              "      <td>0.564788</td>\n",
              "      <td>0.976612</td>\n",
              "      <td>0.513310</td>\n",
              "      <td>-1.008970</td>\n",
              "      <td>-0.251507</td>\n",
              "      <td>-0.222091</td>\n",
              "      <td>0.373584</td>\n",
              "      <td>-0.339755</td>\n",
              "      <td>1.300189</td>\n",
              "      <td>-0.030887</td>\n",
              "      <td>0.219149</td>\n",
              "      <td>1.226649</td>\n",
              "      <td>0.211795</td>\n",
              "      <td>0.542726</td>\n",
              "      <td>-1.722309</td>\n",
              "      <td>0.447124</td>\n",
              "      <td>0.138255</td>\n",
              "      <td>0.998674</td>\n",
              "      <td>-1.031032</td>\n",
              "      <td>0.380938</td>\n",
              "      <td>0.505956</td>\n",
              "      <td>0.322106</td>\n",
              "      <td>-2.237090</td>\n",
              "      <td>0.469186</td>\n",
              "      <td>0.101485</td>\n",
              "      <td>0.050007</td>\n",
              "      <td>0.733930</td>\n",
              "      <td>-0.663332</td>\n",
              "      <td>-0.692748</td>\n",
              "      <td>-2.795994</td>\n",
              "      <td>0.454478</td>\n",
              "      <td>0.285336</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.868428</td>\n",
              "      <td>-1.540314</td>\n",
              "      <td>1.698789</td>\n",
              "      <td>-0.091653</td>\n",
              "      <td>0.744815</td>\n",
              "      <td>-2.165877</td>\n",
              "      <td>-1.396634</td>\n",
              "      <td>1.072285</td>\n",
              "      <td>-0.878596</td>\n",
              "      <td>1.271327</td>\n",
              "      <td>-1.049581</td>\n",
              "      <td>-0.435882</td>\n",
              "      <td>1.109570</td>\n",
              "      <td>1.818930</td>\n",
              "      <td>-0.502920</td>\n",
              "      <td>1.304470</td>\n",
              "      <td>1.143654</td>\n",
              "      <td>0.772685</td>\n",
              "      <td>-0.792163</td>\n",
              "      <td>-0.970303</td>\n",
              "      <td>-0.014635</td>\n",
              "      <td>0.104753</td>\n",
              "      <td>-0.570711</td>\n",
              "      <td>-1.535606</td>\n",
              "      <td>-0.234768</td>\n",
              "      <td>-1.033575</td>\n",
              "      <td>0.613941</td>\n",
              "      <td>1.133485</td>\n",
              "      <td>0.445027</td>\n",
              "      <td>-1.301538</td>\n",
              "      <td>0.551799</td>\n",
              "      <td>1.066635</td>\n",
              "      <td>0.126409</td>\n",
              "      <td>-1.462542</td>\n",
              "      <td>1.177361</td>\n",
              "      <td>-1.142417</td>\n",
              "      <td>-0.506686</td>\n",
              "      <td>-0.741696</td>\n",
              "      <td>0.565734</td>\n",
              "      <td>-1.304175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>M052cFV0RUdYT3FRMU1Kab7l27PQanylu/gYY6UjtK0=</td>\n",
              "      <td>16</td>\n",
              "      <td>0.968543</td>\n",
              "      <td>0.253863</td>\n",
              "      <td>0.799112</td>\n",
              "      <td>-1.399473</td>\n",
              "      <td>-1.979899</td>\n",
              "      <td>0.086771</td>\n",
              "      <td>-1.214792</td>\n",
              "      <td>-0.423301</td>\n",
              "      <td>1.159680</td>\n",
              "      <td>0.632020</td>\n",
              "      <td>-0.599188</td>\n",
              "      <td>2.311739</td>\n",
              "      <td>-0.845429</td>\n",
              "      <td>-1.188409</td>\n",
              "      <td>-0.229825</td>\n",
              "      <td>1.458688</td>\n",
              "      <td>-1.522593</td>\n",
              "      <td>0.869467</td>\n",
              "      <td>0.799112</td>\n",
              "      <td>0.728758</td>\n",
              "      <td>-0.370535</td>\n",
              "      <td>0.596843</td>\n",
              "      <td>-0.933373</td>\n",
              "      <td>-0.423301</td>\n",
              "      <td>1.230035</td>\n",
              "      <td>-0.581599</td>\n",
              "      <td>0.368190</td>\n",
              "      <td>1.564220</td>\n",
              "      <td>-0.150676</td>\n",
              "      <td>-1.610537</td>\n",
              "      <td>-0.053939</td>\n",
              "      <td>-1.707275</td>\n",
              "      <td>0.500105</td>\n",
              "      <td>-0.335358</td>\n",
              "      <td>1.696135</td>\n",
              "      <td>-0.194648</td>\n",
              "      <td>-0.906990</td>\n",
              "      <td>-0.616776</td>\n",
              "      <td>...</td>\n",
              "      <td>1.394487</td>\n",
              "      <td>0.030902</td>\n",
              "      <td>-1.139577</td>\n",
              "      <td>-0.811661</td>\n",
              "      <td>-1.481612</td>\n",
              "      <td>0.080544</td>\n",
              "      <td>2.045310</td>\n",
              "      <td>-1.033460</td>\n",
              "      <td>0.249057</td>\n",
              "      <td>-0.999302</td>\n",
              "      <td>-0.214580</td>\n",
              "      <td>0.009951</td>\n",
              "      <td>-1.317654</td>\n",
              "      <td>-0.426815</td>\n",
              "      <td>1.238271</td>\n",
              "      <td>-0.749721</td>\n",
              "      <td>-0.384915</td>\n",
              "      <td>-1.393713</td>\n",
              "      <td>-0.292916</td>\n",
              "      <td>1.823055</td>\n",
              "      <td>-0.705544</td>\n",
              "      <td>-1.016609</td>\n",
              "      <td>0.808336</td>\n",
              "      <td>1.624028</td>\n",
              "      <td>1.222331</td>\n",
              "      <td>1.362606</td>\n",
              "      <td>0.601567</td>\n",
              "      <td>-0.518814</td>\n",
              "      <td>-0.149908</td>\n",
              "      <td>-1.030272</td>\n",
              "      <td>0.741387</td>\n",
              "      <td>-0.838532</td>\n",
              "      <td>-0.137156</td>\n",
              "      <td>0.075079</td>\n",
              "      <td>-1.395079</td>\n",
              "      <td>2.381879</td>\n",
              "      <td>-0.934174</td>\n",
              "      <td>0.775545</td>\n",
              "      <td>1.014650</td>\n",
              "      <td>1.978815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>M0FFZm51b3d2a2xZOWFRWEkjj3iGrBdIeJj7omGlFz0=</td>\n",
              "      <td>4</td>\n",
              "      <td>1.006677</td>\n",
              "      <td>-0.262913</td>\n",
              "      <td>0.091079</td>\n",
              "      <td>0.799063</td>\n",
              "      <td>0.998184</td>\n",
              "      <td>-0.804963</td>\n",
              "      <td>-1.468698</td>\n",
              "      <td>-0.041668</td>\n",
              "      <td>0.533569</td>\n",
              "      <td>1.053495</td>\n",
              "      <td>0.290200</td>\n",
              "      <td>0.013643</td>\n",
              "      <td>0.345511</td>\n",
              "      <td>-1.900126</td>\n",
              "      <td>-1.988624</td>\n",
              "      <td>0.301262</td>\n",
              "      <td>-0.849212</td>\n",
              "      <td>0.522507</td>\n",
              "      <td>1.617670</td>\n",
              "      <td>0.987121</td>\n",
              "      <td>0.234888</td>\n",
              "      <td>0.212764</td>\n",
              "      <td>0.367635</td>\n",
              "      <td>-0.318224</td>\n",
              "      <td>0.721627</td>\n",
              "      <td>-0.141228</td>\n",
              "      <td>-2.021811</td>\n",
              "      <td>1.009246</td>\n",
              "      <td>-0.395660</td>\n",
              "      <td>0.788001</td>\n",
              "      <td>1.374300</td>\n",
              "      <td>0.799063</td>\n",
              "      <td>-1.634632</td>\n",
              "      <td>1.484923</td>\n",
              "      <td>0.622067</td>\n",
              "      <td>-1.103644</td>\n",
              "      <td>-0.318224</td>\n",
              "      <td>-0.981959</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.955351</td>\n",
              "      <td>-0.060545</td>\n",
              "      <td>0.819685</td>\n",
              "      <td>0.169197</td>\n",
              "      <td>0.017889</td>\n",
              "      <td>-0.483210</td>\n",
              "      <td>-0.861958</td>\n",
              "      <td>1.730791</td>\n",
              "      <td>-1.172436</td>\n",
              "      <td>0.075229</td>\n",
              "      <td>-0.269001</td>\n",
              "      <td>-0.851602</td>\n",
              "      <td>0.139472</td>\n",
              "      <td>0.791303</td>\n",
              "      <td>-2.172907</td>\n",
              "      <td>1.119041</td>\n",
              "      <td>1.102165</td>\n",
              "      <td>-0.102160</td>\n",
              "      <td>-1.531049</td>\n",
              "      <td>-1.397192</td>\n",
              "      <td>-0.206292</td>\n",
              "      <td>-0.411104</td>\n",
              "      <td>-1.535843</td>\n",
              "      <td>-0.479374</td>\n",
              "      <td>-1.389521</td>\n",
              "      <td>-1.456641</td>\n",
              "      <td>0.644982</td>\n",
              "      <td>1.625509</td>\n",
              "      <td>1.124794</td>\n",
              "      <td>0.432307</td>\n",
              "      <td>1.197475</td>\n",
              "      <td>1.162764</td>\n",
              "      <td>-0.773935</td>\n",
              "      <td>-0.962255</td>\n",
              "      <td>0.161526</td>\n",
              "      <td>-0.722540</td>\n",
              "      <td>0.490989</td>\n",
              "      <td>-1.036087</td>\n",
              "      <td>1.361248</td>\n",
              "      <td>-0.548412</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 363 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             id  ...  feature_59_type5\n",
              "0  M00yaXQwMVJ1R1Q0RHFBVVR5BHyNyIeEd7yqxidFW7U=  ...         -0.128554\n",
              "1  M01HU2pzN1dZNnVOYmhjayLgjCmIG1AyHvfRsZv+IV4=  ...          0.420162\n",
              "2  M01la1Znd0ROMUU5UGJvdc5EJqBI/dj9r8C86lWkehA=  ...         -1.304175\n",
              "3  M052cFV0RUdYT3FRMU1Kab7l27PQanylu/gYY6UjtK0=  ...          1.978815\n",
              "4  M0FFZm51b3d2a2xZOWFRWEkjj3iGrBdIeJj7omGlFz0=  ...         -0.548412\n",
              "\n",
              "[5 rows x 363 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9Ydt_sDI1N3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=pd.get_dummies(df, columns=['span'],drop_first=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-Q7DvbQC1HV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ebbjf3cfC2Y-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lb=LabelEncoder()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sVAzXXdC2iw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['id']=lb.fit_transform(df.id)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1AZKo1YBZkQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = df[df['target'].isnull()!=True]\n",
        "test_df = df[df['target'].isnull()==True]\n",
        "test_df.drop('target', axis=1, inplace=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPlvOTfSYJun",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df=train_df.fillna(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8gi6aXGYJ4N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df=test_df.fillna(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_D1oXUX7B57x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6691e76f-a161-4f19-c668-fd8fa77da2d2"
      },
      "source": [
        "train_df.shape, test_df.shape"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4177, 394), (1000, 393))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "26W_ubPLNtgg",
        "colab": {}
      },
      "source": [
        "#train_df['target'] = np.log1p(train_df['target'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vf4PqnNwCCUX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = train_df.drop(labels=['target'], axis=1)\n",
        "y = train_df['target'].values\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_aeXqso3Cir",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "rf = RandomForestRegressor(n_estimators=300, random_state=1)\n",
        "rf.fit(X, y)\n",
        "y_pred_rf = rf.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuazuzU05kxI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import mean_squared_log_error,mean_squared_error,mean_absolute_error\n",
        "from math import sqrt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPwYiV0I68hI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVLka3Ib4aZo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5ef6bdf3-ecbb-4499-99e6-a02dfced162c"
      },
      "source": [
        "print('RMSE:', rmse(y_test,y_pred_rf))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSLE: 0.005451516347942205\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlrZJGOq5jbD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4de6c42b-5cc4-441e-f7b2-f63ebf81d6d3"
      },
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "br = BaggingRegressor( n_estimators=300, random_state=1)\n",
        "br.fit(X, y)\n",
        "y_pred_br = br.predict(X_test)\n",
        "print('RMSE:', rmse(y_test,y_pred_br))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE: 0.005457644479035296\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBBcSd2tKsTt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "e63076da-fe56-433e-d84c-767f9d7adfec"
      },
      "source": [
        "!pip install catboost"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting catboost\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/f6/733fe7cca5d0d882e1a708ad59da2510416cc2e4fa54e17c7a5082f67811/catboost-0.20.1-cp36-none-manylinux1_x86_64.whl (63.6MB)\n",
            "\u001b[K     |████████████████████████████████| 63.6MB 125kB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from catboost) (3.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from catboost) (1.3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from catboost) (1.12.0)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.6/dist-packages (from catboost) (0.25.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (from catboost) (4.1.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.17.4)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (2.4.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (2.6.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.0->catboost) (2018.9)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly->catboost) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->catboost) (42.0.2)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-0.20.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BKZHbDaSigE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from catboost import CatBoostRegressor\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEgW5lzUk6y4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "categorical_features_indices = np.where(X_train.dtypes == 'category')[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AsB0ePJSqrE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "4c727ec6-8471-4c6a-f28f-07a23ce15bcc"
      },
      "source": [
        "model =  CatBoostRegressor(iterations=2000,\n",
        "                             learning_rate=0.01,\n",
        "                             depth=10,\n",
        "                             eval_metric='RMSE',\n",
        "                             random_seed = 42,\n",
        "                             bagging_temperature = 0.2,\n",
        "                             od_type='Iter',\n",
        "                             metric_period = 75,\n",
        "                             od_wait=100)\n",
        "model.fit(X, y,\n",
        "                 eval_set=(X_test, y_test),\n",
        "                 cat_features=categorical_features_indices,\n",
        "                 use_best_model=True)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Overfitting detector is active, thus evaluation metric is calculated on every iteration. 'metric_period' is ignored for evaluation metric.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0:\tlearn: 0.0201278\ttest: 0.0206973\tbest: 0.0206973 (0)\ttotal: 877ms\tremaining: 29m 12s\n",
            "75:\tlearn: 0.0176130\ttest: 0.0181101\tbest: 0.0181101 (75)\ttotal: 1m 5s\tremaining: 27m 45s\n",
            "150:\tlearn: 0.0158078\ttest: 0.0162647\tbest: 0.0162647 (150)\ttotal: 2m 10s\tremaining: 26m 39s\n",
            "225:\tlearn: 0.0143696\ttest: 0.0148132\tbest: 0.0148132 (225)\ttotal: 3m 15s\tremaining: 25m 34s\n",
            "300:\tlearn: 0.0131488\ttest: 0.0135695\tbest: 0.0135695 (300)\ttotal: 4m 20s\tremaining: 24m 29s\n",
            "375:\tlearn: 0.0120621\ttest: 0.0124828\tbest: 0.0124828 (375)\ttotal: 5m 25s\tremaining: 23m 24s\n",
            "450:\tlearn: 0.0111257\ttest: 0.0115309\tbest: 0.0115309 (450)\ttotal: 6m 30s\tremaining: 22m 19s\n",
            "525:\tlearn: 0.0102825\ttest: 0.0106888\tbest: 0.0106888 (525)\ttotal: 7m 34s\tremaining: 21m 15s\n",
            "600:\tlearn: 0.0095228\ttest: 0.0099210\tbest: 0.0099210 (600)\ttotal: 8m 39s\tremaining: 20m 10s\n",
            "675:\tlearn: 0.0088332\ttest: 0.0092310\tbest: 0.0092310 (675)\ttotal: 9m 45s\tremaining: 19m 5s\n",
            "750:\tlearn: 0.0082474\ttest: 0.0086518\tbest: 0.0086518 (750)\ttotal: 10m 49s\tremaining: 18m\n",
            "825:\tlearn: 0.0076826\ttest: 0.0080709\tbest: 0.0080709 (825)\ttotal: 11m 54s\tremaining: 16m 55s\n",
            "900:\tlearn: 0.0071731\ttest: 0.0075542\tbest: 0.0075542 (900)\ttotal: 12m 59s\tremaining: 15m 50s\n",
            "975:\tlearn: 0.0067056\ttest: 0.0070730\tbest: 0.0070730 (975)\ttotal: 14m 3s\tremaining: 14m 45s\n",
            "1050:\tlearn: 0.0062695\ttest: 0.0066181\tbest: 0.0066181 (1050)\ttotal: 15m 8s\tremaining: 13m 40s\n",
            "1125:\tlearn: 0.0058846\ttest: 0.0062201\tbest: 0.0062201 (1125)\ttotal: 16m 12s\tremaining: 12m 34s\n",
            "1200:\tlearn: 0.0055020\ttest: 0.0058312\tbest: 0.0058312 (1200)\ttotal: 17m 17s\tremaining: 11m 30s\n",
            "1275:\tlearn: 0.0051811\ttest: 0.0054979\tbest: 0.0054979 (1275)\ttotal: 18m 21s\tremaining: 10m 25s\n",
            "1350:\tlearn: 0.0048669\ttest: 0.0051740\tbest: 0.0051740 (1350)\ttotal: 19m 26s\tremaining: 9m 20s\n",
            "1425:\tlearn: 0.0045577\ttest: 0.0048553\tbest: 0.0048553 (1425)\ttotal: 20m 30s\tremaining: 8m 15s\n",
            "1500:\tlearn: 0.0042873\ttest: 0.0045720\tbest: 0.0045720 (1500)\ttotal: 21m 35s\tremaining: 7m 10s\n",
            "1575:\tlearn: 0.0040359\ttest: 0.0043095\tbest: 0.0043095 (1575)\ttotal: 22m 40s\tremaining: 6m 5s\n",
            "1650:\tlearn: 0.0038160\ttest: 0.0040779\tbest: 0.0040779 (1650)\ttotal: 23m 44s\tremaining: 5m 1s\n",
            "1725:\tlearn: 0.0036044\ttest: 0.0038544\tbest: 0.0038544 (1725)\ttotal: 24m 49s\tremaining: 3m 56s\n",
            "1800:\tlearn: 0.0034098\ttest: 0.0036488\tbest: 0.0036488 (1800)\ttotal: 25m 53s\tremaining: 2m 51s\n",
            "1875:\tlearn: 0.0032318\ttest: 0.0034591\tbest: 0.0034591 (1875)\ttotal: 26m 58s\tremaining: 1m 46s\n",
            "1950:\tlearn: 0.0030614\ttest: 0.0032795\tbest: 0.0032795 (1950)\ttotal: 28m 3s\tremaining: 42.3s\n",
            "1999:\tlearn: 0.0029606\ttest: 0.0031674\tbest: 0.0031674 (1999)\ttotal: 28m 45s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.003167384466\n",
            "bestIteration = 1999\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<catboost.core.CatBoostRegressor at 0x7f10d034fef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60qXCkG0ZgdD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "94e1b86a-88c2-4e17-fcb9-951d685524cf"
      },
      "source": [
        "y_pred_cat = model.predict(X_test)\n",
        "print('RMSE:', rmse(y_test,y_pred_cat))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE: 0.003167384101263437\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8XeXD0VkuPQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "41e97527-7d27-4be9-e2e7-2790fd5a84bc"
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "gb = GradientBoostingRegressor(loss='ls', \n",
        "                               learning_rate=0.3, \n",
        "                               n_estimators=300, \n",
        "                               subsample=1.0, \n",
        "                               criterion='friedman_mse', \n",
        "                               min_samples_split=30, \n",
        "                               min_samples_leaf=1, \n",
        "                               min_weight_fraction_leaf=0.0, \n",
        "                               max_depth=7, \n",
        "                               min_impurity_decrease=0.0, \n",
        "                               min_impurity_split=None, \n",
        "                               init=None, \n",
        "                               random_state=0, \n",
        "                               max_features=None, \n",
        "                               alpha=0.9, \n",
        "                               verbose=100, \n",
        "                               max_leaf_nodes=None, \n",
        "                               warm_start=False, \n",
        "                               presort='auto')\n",
        "gb.fit(X, y)\n",
        "y_pred4 = gb.predict(X_test)\n",
        "print('RMSE:', rmse(y_test,y_pred4))\n"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      Iter       Train Loss   Remaining Time \n",
            "         1           0.0004            1.68m\n",
            "         2           0.0003            1.87m\n",
            "         3           0.0002            1.87m\n",
            "         4           0.0002            1.75m\n",
            "         5           0.0002            1.77m\n",
            "         6           0.0002            1.79m\n",
            "         7           0.0002            1.75m\n",
            "         8           0.0001            1.74m\n",
            "         9           0.0001            1.72m\n",
            "        10           0.0001            1.67m\n",
            "        11           0.0001            1.66m\n",
            "        12           0.0001            1.64m\n",
            "        13           0.0001            1.61m\n",
            "        14           0.0001            1.60m\n",
            "        15           0.0001            1.56m\n",
            "        16           0.0001            1.54m\n",
            "        17           0.0001            1.52m\n",
            "        18           0.0001            1.50m\n",
            "        19           0.0001            1.47m\n",
            "        20           0.0001            1.46m\n",
            "        21           0.0001            1.47m\n",
            "        22           0.0001            1.48m\n",
            "        23           0.0001            1.48m\n",
            "        24           0.0001            1.46m\n",
            "        25           0.0001            1.45m\n",
            "        26           0.0001            1.43m\n",
            "        27           0.0001            1.42m\n",
            "        28           0.0001            1.40m\n",
            "        29           0.0001            1.39m\n",
            "        30           0.0000            1.40m\n",
            "        31           0.0000            1.40m\n",
            "        32           0.0000            1.39m\n",
            "        33           0.0000            1.38m\n",
            "        34           0.0000            1.38m\n",
            "        35           0.0000            1.37m\n",
            "        36           0.0000            1.36m\n",
            "        37           0.0000            1.35m\n",
            "        38           0.0000            1.35m\n",
            "        39           0.0000            1.33m\n",
            "        40           0.0000            1.32m\n",
            "        41           0.0000            1.31m\n",
            "        42           0.0000            1.30m\n",
            "        43           0.0000            1.30m\n",
            "        44           0.0000            1.30m\n",
            "        45           0.0000            1.30m\n",
            "        46           0.0000            1.30m\n",
            "        47           0.0000            1.30m\n",
            "        48           0.0000            1.29m\n",
            "        49           0.0000            1.28m\n",
            "        50           0.0000            1.27m\n",
            "        51           0.0000            1.26m\n",
            "        52           0.0000            1.26m\n",
            "        53           0.0000            1.25m\n",
            "        54           0.0000            1.24m\n",
            "        55           0.0000            1.23m\n",
            "        56           0.0000            1.23m\n",
            "        57           0.0000            1.23m\n",
            "        58           0.0000            1.23m\n",
            "        59           0.0000            1.22m\n",
            "        60           0.0000            1.21m\n",
            "        61           0.0000            1.20m\n",
            "        62           0.0000            1.19m\n",
            "        63           0.0000            1.19m\n",
            "        64           0.0000            1.18m\n",
            "        65           0.0000            1.18m\n",
            "        66           0.0000            1.17m\n",
            "        67           0.0000            1.16m\n",
            "        68           0.0000            1.15m\n",
            "        69           0.0000            1.15m\n",
            "        70           0.0000            1.14m\n",
            "        71           0.0000            1.13m\n",
            "        72           0.0000            1.13m\n",
            "        73           0.0000            1.12m\n",
            "        74           0.0000            1.12m\n",
            "        75           0.0000            1.11m\n",
            "        76           0.0000            1.11m\n",
            "        77           0.0000            1.10m\n",
            "        78           0.0000            1.09m\n",
            "        79           0.0000            1.09m\n",
            "        80           0.0000            1.08m\n",
            "        81           0.0000            1.08m\n",
            "        82           0.0000            1.07m\n",
            "        83           0.0000            1.06m\n",
            "        84           0.0000            1.06m\n",
            "        85           0.0000            1.05m\n",
            "        86           0.0000            1.05m\n",
            "        87           0.0000            1.04m\n",
            "        88           0.0000            1.04m\n",
            "        89           0.0000            1.03m\n",
            "        90           0.0000            1.03m\n",
            "        91           0.0000            1.02m\n",
            "        92           0.0000            1.02m\n",
            "        93           0.0000            1.02m\n",
            "        94           0.0000            1.01m\n",
            "        95           0.0000            1.01m\n",
            "        96           0.0000            1.00m\n",
            "        97           0.0000           59.80s\n",
            "        98           0.0000           59.47s\n",
            "        99           0.0000           59.08s\n",
            "       100           0.0000           58.69s\n",
            "       101           0.0000           58.37s\n",
            "       102           0.0000           57.96s\n",
            "       103           0.0000           57.62s\n",
            "       104           0.0000           57.32s\n",
            "       105           0.0000           57.01s\n",
            "       106           0.0000           56.89s\n",
            "       107           0.0000           56.71s\n",
            "       108           0.0000           56.42s\n",
            "       109           0.0000           56.27s\n",
            "       110           0.0000           55.87s\n",
            "       111           0.0000           55.49s\n",
            "       112           0.0000           55.09s\n",
            "       113           0.0000           54.71s\n",
            "       114           0.0000           54.42s\n",
            "       115           0.0000           54.05s\n",
            "       116           0.0000           53.74s\n",
            "       117           0.0000           53.39s\n",
            "       118           0.0000           53.02s\n",
            "       119           0.0000           52.71s\n",
            "       120           0.0000           52.54s\n",
            "       121           0.0000           52.30s\n",
            "       122           0.0000           52.04s\n",
            "       123           0.0000           51.72s\n",
            "       124           0.0000           51.48s\n",
            "       125           0.0000           51.23s\n",
            "       126           0.0000           50.92s\n",
            "       127           0.0000           50.73s\n",
            "       128           0.0000           50.40s\n",
            "       129           0.0000           50.14s\n",
            "       130           0.0000           49.81s\n",
            "       131           0.0000           49.47s\n",
            "       132           0.0000           49.23s\n",
            "       133           0.0000           48.86s\n",
            "       134           0.0000           48.53s\n",
            "       135           0.0000           48.23s\n",
            "       136           0.0000           47.88s\n",
            "       137           0.0000           47.58s\n",
            "       138           0.0000           47.22s\n",
            "       139           0.0000           46.90s\n",
            "       140           0.0000           46.57s\n",
            "       141           0.0000           46.24s\n",
            "       142           0.0000           45.91s\n",
            "       143           0.0000           45.57s\n",
            "       144           0.0000           45.25s\n",
            "       145           0.0000           44.94s\n",
            "       146           0.0000           44.72s\n",
            "       147           0.0000           44.47s\n",
            "       148           0.0000           44.12s\n",
            "       149           0.0000           43.79s\n",
            "       150           0.0000           43.55s\n",
            "       151           0.0000           43.33s\n",
            "       152           0.0000           43.03s\n",
            "       153           0.0000           42.68s\n",
            "       154           0.0000           42.35s\n",
            "       155           0.0000           42.07s\n",
            "       156           0.0000           41.80s\n",
            "       157           0.0000           41.47s\n",
            "       158           0.0000           41.23s\n",
            "       159           0.0000           40.95s\n",
            "       160           0.0000           40.64s\n",
            "       161           0.0000           40.32s\n",
            "       162           0.0000           40.06s\n",
            "       163           0.0000           39.77s\n",
            "       164           0.0000           39.52s\n",
            "       165           0.0000           39.20s\n",
            "       166           0.0000           38.93s\n",
            "       167           0.0000           38.62s\n",
            "       168           0.0000           38.30s\n",
            "       169           0.0000           37.97s\n",
            "       170           0.0000           37.66s\n",
            "       171           0.0000           37.35s\n",
            "       172           0.0000           37.04s\n",
            "       173           0.0000           36.77s\n",
            "       174           0.0000           36.44s\n",
            "       175           0.0000           36.12s\n",
            "       176           0.0000           35.80s\n",
            "       177           0.0000           35.49s\n",
            "       178           0.0000           35.17s\n",
            "       179           0.0000           34.86s\n",
            "       180           0.0000           34.55s\n",
            "       181           0.0000           34.23s\n",
            "       182           0.0000           33.91s\n",
            "       183           0.0000           33.62s\n",
            "       184           0.0000           33.36s\n",
            "       185           0.0000           33.07s\n",
            "       186           0.0000           32.77s\n",
            "       187           0.0000           32.46s\n",
            "       188           0.0000           32.16s\n",
            "       189           0.0000           31.87s\n",
            "       190           0.0000           31.56s\n",
            "       191           0.0000           31.24s\n",
            "       192           0.0000           30.95s\n",
            "       193           0.0000           30.67s\n",
            "       194           0.0000           30.40s\n",
            "       195           0.0000           30.09s\n",
            "       196           0.0000           29.78s\n",
            "       197           0.0000           29.49s\n",
            "       198           0.0000           29.22s\n",
            "       199           0.0000           28.92s\n",
            "       200           0.0000           28.65s\n",
            "       201           0.0000           28.36s\n",
            "       202           0.0000           28.06s\n",
            "       203           0.0000           27.77s\n",
            "       204           0.0000           27.48s\n",
            "       205           0.0000           27.20s\n",
            "       206           0.0000           26.91s\n",
            "       207           0.0000           26.64s\n",
            "       208           0.0000           26.36s\n",
            "       209           0.0000           26.06s\n",
            "       210           0.0000           25.77s\n",
            "       211           0.0000           25.51s\n",
            "       212           0.0000           25.21s\n",
            "       213           0.0000           24.91s\n",
            "       214           0.0000           24.61s\n",
            "       215           0.0000           24.31s\n",
            "       216           0.0000           24.01s\n",
            "       217           0.0000           23.70s\n",
            "       218           0.0000           23.44s\n",
            "       219           0.0000           23.14s\n",
            "       220           0.0000           22.84s\n",
            "       221           0.0000           22.54s\n",
            "       222           0.0000           22.25s\n",
            "       223           0.0000           21.94s\n",
            "       224           0.0000           21.66s\n",
            "       225           0.0000           21.38s\n",
            "       226           0.0000           21.09s\n",
            "       227           0.0000           20.79s\n",
            "       228           0.0000           20.49s\n",
            "       229           0.0000           20.21s\n",
            "       230           0.0000           19.93s\n",
            "       231           0.0000           19.64s\n",
            "       232           0.0000           19.35s\n",
            "       233           0.0000           19.06s\n",
            "       234           0.0000           18.77s\n",
            "       235           0.0000           18.50s\n",
            "       236           0.0000           18.21s\n",
            "       237           0.0000           17.93s\n",
            "       238           0.0000           17.64s\n",
            "       239           0.0000           17.35s\n",
            "       240           0.0000           17.06s\n",
            "       241           0.0000           16.77s\n",
            "       242           0.0000           16.48s\n",
            "       243           0.0000           16.20s\n",
            "       244           0.0000           15.90s\n",
            "       245           0.0000           15.61s\n",
            "       246           0.0000           15.34s\n",
            "       247           0.0000           15.07s\n",
            "       248           0.0000           14.80s\n",
            "       249           0.0000           14.50s\n",
            "       250           0.0000           14.22s\n",
            "       251           0.0000           13.93s\n",
            "       252           0.0000           13.63s\n",
            "       253           0.0000           13.34s\n",
            "       254           0.0000           13.05s\n",
            "       255           0.0000           12.77s\n",
            "       256           0.0000           12.48s\n",
            "       257           0.0000           12.19s\n",
            "       258           0.0000           11.90s\n",
            "       259           0.0000           11.61s\n",
            "       260           0.0000           11.32s\n",
            "       261           0.0000           11.03s\n",
            "       262           0.0000           10.74s\n",
            "       263           0.0000           10.45s\n",
            "       264           0.0000           10.17s\n",
            "       265           0.0000            9.89s\n",
            "       266           0.0000            9.60s\n",
            "       267           0.0000            9.32s\n",
            "       268           0.0000            9.03s\n",
            "       269           0.0000            8.75s\n",
            "       270           0.0000            8.46s\n",
            "       271           0.0000            8.15s\n",
            "       272           0.0000            7.84s\n",
            "       273           0.0000            7.53s\n",
            "       274           0.0000            7.22s\n",
            "       275           0.0000            6.92s\n",
            "       276           0.0000            6.62s\n",
            "       277           0.0000            6.32s\n",
            "       278           0.0000            6.02s\n",
            "       279           0.0000            5.73s\n",
            "       280           0.0000            5.44s\n",
            "       281           0.0000            5.15s\n",
            "       282           0.0000            4.86s\n",
            "       283           0.0000            4.57s\n",
            "       284           0.0000            4.29s\n",
            "       285           0.0000            4.01s\n",
            "       286           0.0000            3.73s\n",
            "       287           0.0000            3.45s\n",
            "       288           0.0000            3.17s\n",
            "       289           0.0000            2.90s\n",
            "       290           0.0000            2.63s\n",
            "       291           0.0000            2.35s\n",
            "       292           0.0000            2.09s\n",
            "       293           0.0000            1.82s\n",
            "       294           0.0000            1.55s\n",
            "       295           0.0000            1.29s\n",
            "       296           0.0000            1.03s\n",
            "       297           0.0000            0.77s\n",
            "       298           0.0000            0.51s\n",
            "       299           0.0000            0.25s\n",
            "       300           0.0000            0.00s\n",
            "RMSE: 0.00032372055724967945\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZEjuwIat9r0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "9bb85f1b-c122-4b87-c951-b08106eeae83"
      },
      "source": [
        "import lightgbm as lgb\n",
        "train_data = lgb.Dataset(X, label=y)\n",
        "test_data = lgb.Dataset(X_test, label=y_test)\n",
        "\n",
        "param = {'objective': 'regression',\n",
        "         'boosting': 'gbdt',\n",
        "         'num_iterations': 3000,   \n",
        "         'learning_rate': 0.06,  \n",
        "         'num_leaves': 40,  \n",
        "         'max_depth': 24,   \n",
        "         'min_data_in_leaf':11,  \n",
        "         'max_bin': 4, \n",
        "         'metric': 'l2_root'\n",
        "         }\n",
        "\n",
        "lgbm = lgb.train(params=param,\n",
        "                 verbose_eval=1000,\n",
        "                 train_set=train_data,\n",
        "                 valid_sets=[test_data])\n",
        "\n",
        "y_pred2 = lgbm.predict(X_test)\n",
        "print('RMSE:', rmse(y_test,y_pred2))\n"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1000]\tvalid_0's rmse: 0.000353401\n",
            "[2000]\tvalid_0's rmse: 2.3564e-05\n",
            "[3000]\tvalid_0's rmse: 1.6616e-06\n",
            "RMSE: 1.6618369485968043e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5ZBwUKju5sm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "286f0e2a-ebf3-4fe2-b4b4-677fd8c2a7c2"
      },
      "source": [
        "y_pred = y_pred_rf*0.00 + y_pred_br*0.00 + y_pred2*0.50 + y_pred_cat*0.00 + y_pred4*0.50\n",
        "print('RMSE:', rmse(y_test,y_pred))\n",
        "\n"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE: 0.00016201435652049266\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vtg-E89fvreU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "27a4403e-348f-443f-e74f-ac75c4a5040b"
      },
      "source": [
        "y_pred"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.00045554, 1.00652168, 0.97346712, ..., 0.96348248, 0.99427722,\n",
              "       1.04142504])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JK38ww0FwrJk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}